import os

import openai
from typing import Literal, Iterator, Optional, Any

from .exceptions import LLMAgentException

SUPPORTED_PROVIDERS = Literal[
    "openai",
    "intern",
    "zhipu",
    "local"
]


class VerAgentLLM:
    """
    å‚æ•°åŠ è½½ç­–ç•¥ï¼š
        1. ä»å‚æ•°ä¸­è·å–providerï¼Œmodelï¼Œapi_keyï¼Œbase_urlç­‰
        2. ä»ç¯å¢ƒå˜é‡ä¸­è·å–è¿™äº›ä¿¡æ¯
        3. å¦‚æœå‚æ•°ä¸­æ²¡æœ‰æä¾›ï¼Œä¹Ÿæ²¡æœ‰ä»ç¯å¢ƒå˜é‡ä¸­è·å–åˆ°ï¼ŒæŠ›å‡ºå¼‚å¸¸
    """

    def __init__(
        self,
        provider: Optional[SUPPORTED_PROVIDERS] = None,
        model: Optional[str] = None,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        temperature: Optional[float] = 0.7,
        max_tokens: Optional[int] = None,
        timeout: Optional[int] = None,
        **kwargs
    ):
        self.provider = (provider or os.environ.get("PROVIDER")).lower()
        self.model = model or os.environ.get(f"{self.provider.upper()}_MODEL") or os.environ.get("MODEL")
        self.api_key = api_key or os.environ.get(f"{self.provider.upper()}_API_KEY") or os.environ.get("API_KEY")
        self.base_url = base_url or os.environ.get(f"{self.provider.upper()}_BASE_URL") or os.environ.get("BASE_URL")

        if not self.api_key:
            raise LLMAgentException(f"api_key is required for {self.provider}")
        if not self.base_url:
            raise LLMAgentException(f"base_url is required for {self.provider}")

        if not self.model:
            raise LLMAgentException(f"model is required for {self.provider}")

        self.temperature = temperature
        self.max_tokens = max_tokens
        self.timeout = timeout

        self._client = openai.OpenAI(
            api_key=self.api_key,
            base_url=self.base_url,
            timeout=self.timeout,
        )

    def think(
        self,
        messages: list[dict[str, str]],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        quiet: bool = False,
    ) -> Iterator[str]:
        """
        stream response

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°
            quiet: æ˜¯å¦é™é»˜æ¨¡å¼ï¼ˆä¸è¾“å‡º "thinking..." æ¶ˆæ¯ï¼‰

        Returns:
            æµå¼å“åº”çš„è¿­ä»£å™¨
        """
        if not quiet:
            print(f"\nğŸ¤” {self.model} is thinking...\n")

        try:
            response = self._client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature or self.temperature,
                max_tokens=max_tokens or self.max_tokens,
                stream=True
            )
            for chunk in response:
                # print(chunk,end="",flush=True)
                yield chunk.choices[0].delta.content or ""
        except Exception as e:
            raise LLMAgentException(f"âŒ ERROR: {e}")

        # print(f"ğŸ‘{self.model} is done thinking!")

    def invoke(
        self,
        message: list[dict[str, str]],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        tools: Optional[list[dict[str, Any]]] = None,
        **kwargs
    ) -> dict[str, Any]:
        """
        è°ƒç”¨ LLM å¹¶è¿”å›å®Œæ•´å“åº”ï¼ˆåŒ…å« tool_callsï¼‰

        Args:
            message: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°
            tools: OpenAI å·¥å…·åˆ—è¡¨æ ¼å¼
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            åŒ…å« content å’Œ tool_calls çš„å“åº”å­—å…¸
        """
        try:
            api_kwargs = {
                "model": self.model,
                "messages": message,
                "temperature": temperature or self.temperature,
                "max_tokens": max_tokens or self.max_tokens,
            }

            # æ·»åŠ  tools å‚æ•°ï¼ˆå¦‚æœæä¾›ï¼‰
            if tools:
                api_kwargs["tools"] = tools

            # æ·»åŠ å…¶ä»– kwargs
            for k, v in kwargs.items():
                if k not in ['temperature', 'max_tokens', 'tools']:
                    api_kwargs[k] = v

            response = self._client.chat.completions.create(**api_kwargs)

            result = {
                "content": response.choices[0].message.content or "",
                "tool_calls": response.choices[0].message.tool_calls
            }

            return result
        except Exception as e:
            raise LLMAgentException(f"âŒ ERROR: {e}")
